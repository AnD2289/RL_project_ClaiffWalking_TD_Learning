{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[toy_text] in /home/amitdutta/anaconda3/lib/python3.8/site-packages (0.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/amitdutta/anaconda3/lib/python3.8/site-packages (from gym[toy_text]) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/amitdutta/.local/lib/python3.8/site-packages (from gym[toy_text]) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.4.1; extra == \"toy_text\" in /home/amitdutta/anaconda3/lib/python3.8/site-packages (from gym[toy_text]) (1.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q gym\n",
    "!pip install -q matplotlib\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "! pip install gym[toy_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_policy_gradient(env, theta, lr, episodes):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    env -- Environment\n",
    "    theta -- The policy function parameter\n",
    "    lr -- Learning rate\n",
    "    episodes -- The number of iterations\n",
    "    Returns: \n",
    "    episodes -- The cumulative reward value\n",
    "    \"\"\"\n",
    "    for episode in range(episodes):  # Iterations\n",
    "        episode = []\n",
    "        start_observation = env.reset()  # Initialize the environment\n",
    "        t = 0\n",
    "        while True:\n",
    "            policy = np.dot(theta, start_observation)  # Calculate the policy value\n",
    "            # Here action_space is 2, so use Sigmoid\n",
    "            pi = 1 / (1 + np.exp(-policy))\n",
    "            if pi >= 0.5:\n",
    "                action = 1  # Push cart to the right\n",
    "            else:\n",
    "                action = 0  # Push cart to the left\n",
    "            next_observation, reward, done, _ = env.step(action)  # Take actions\n",
    "            # Add the environment return result to the episode\n",
    "            episode.append([next_observation, action, pi, reward])\n",
    "            start_observation = next_observation  # Return observation as the next iteration observation\n",
    "            t += 1\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\".format(t))\n",
    "                break\n",
    "        # Update theta\n",
    "        for timestep in episode:\n",
    "            observation, action, pi, reward = timestep\n",
    "            theta += lr * (1 - pi) * np.transpose(-observation) * reward\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.277272186736653\n",
      "7.873734104085031\n",
      "-7.130863119460896\n",
      "-11.145595562684864\n",
      "-4.077233592925907\n",
      "-11.05338301717643\n",
      "-5.739553474021708\n",
      "9.257281120178508\n",
      "-12.296368431835589\n",
      "-1.6037102018486\n",
      "-6.425218290060992\n",
      "-10.260489211646199\n",
      "0.35377581356121657\n",
      "-4.95835546364285\n",
      "-7.829349097017905\n",
      "7.044409658907031\n",
      "10.900065259683236\n",
      "2.7979598667261922\n",
      "-10.265488925033676\n",
      "11.32801475715933\n",
      "2.5451825822224445\n",
      "-13.406758140050448\n",
      "12.978774200275675\n",
      "-23.197671663647284\n",
      "-17.949642975670535\n",
      "7.928659011632703\n",
      "0.6027130784891805\n",
      "-4.50518775575595\n"
     ]
    }
   ],
   "source": [
    "# # environment = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "# env = gym.make('FrozenLake8x8-v0', is_slippery= True)\n",
    "# env.reset()\n",
    "# # environment.render()\n",
    "# mu, sigma = 0, 8 # mean and standard deviation\n",
    "# k=0\n",
    "# # while k<64:\n",
    "# #     action=env.action_space.sample()\n",
    "# # #     print(action)\n",
    "# #     next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "# #     if (next_state == 20,30,64,42,43,47,50,53,55,60):\n",
    "# #         reward = np.random.normal(mu, sigma)\n",
    "# #     else:\n",
    "# #         reward = reward       \n",
    "        \n",
    "# # #     print(next_state)\n",
    "# # #     print(reward)\n",
    "# # #     action=env.action_space.sample()\n",
    "# # #     env.step(action)\n",
    "# # #     env.render()\n",
    "\n",
    "# #     k = k+1\n",
    "# for _ in range(1):\n",
    "#     t = 0\n",
    "#     env.reset()\n",
    "#     while True:\n",
    "#         action = env.action_space.sample()\n",
    "#         observation, reward, done, _ = env.step(action)    \n",
    "#         if (observation == 19,29,35,41,42,46,49,52,54,59):\n",
    "#             reward = np.random.normal(mu, sigma) \n",
    "#         else:\n",
    "#             reward = reward  \n",
    "# #         print(reward)                     \n",
    "#         t += 1\n",
    "#         if done:\n",
    "#             #int(\"Episode finished after {} timesteps\".format(t))\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 12 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 19 timesteps\n",
      "Episode finished after 19 timesteps\n",
      "Episode finished after 17 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 31 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 19 timesteps\n",
      "Episode finished after 7 timesteps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.91575767])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.001\n",
    "theta = np.random.rand(1)\n",
    "episodes=10\n",
    "mc_policy_gradient(env, theta, lr, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3563a4d39a0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;31m# Here action_space is 2, so use Sigmoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mpi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# Push cart to the right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "for episode in range(episodes):  # Iterations\n",
    "        episode = []\n",
    "        start_observation = env.reset()  # Initialize the environment\n",
    "        t = 0\n",
    "        while True:\n",
    "            policy = np.dot(theta, start_observation)  # Calculate the policy value\n",
    "            # Here action_space is 2, so use Sigmoid\n",
    "            pi = 1 / (1 + np.exp(-policy))\n",
    "            if pi >= 0.5:\n",
    "                action = 1  # Push cart to the right\n",
    "            else:\n",
    "                action = 0  # Push cart to the left\n",
    "            next_observation, reward, done, _ = env.step(action)  # Take actions\n",
    "            # Add the environment return result to the episode\n",
    "            episode.append([next_observation, action, pi, reward])\n",
    "            start_observation = next_observation  # Return observation as the next iteration observation\n",
    "            t += 1\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\".format(t))\n",
    "                break\n",
    "        # Update theta\n",
    "        for timestep in episode:\n",
    "            observation, action, pi, reward = timestep\n",
    "            theta += lr * (1 - pi) * np.transpose(-observation) * reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.5 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
